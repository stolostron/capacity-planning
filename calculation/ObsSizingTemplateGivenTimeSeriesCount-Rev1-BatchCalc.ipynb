{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import os\n",
    "import seaborn as sns\n",
    "# Set default Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "#from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def roundup(x):\n",
    "    return int(math.ceil(x / 10.0)) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is still experimental\n",
    "\n",
    "This notebook allows you to calculate the **ACM Hub Observability** sizing for fleet management based on your inputs as explained in __Critical Input Parameters to Size__ section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology of Calculation\n",
    "1. We have obtained the number of time series per cluster by running https://github.com/stolostron/multicluster-observability-operator/tree/main/tools/simulator/metrics-collector/metrics-extractor\n",
    "\n",
    "1. From number of time series AND number of clusters, we infer\n",
    "    1. Memory requirement (2 hours of this time series data is stored in memory)\n",
    "    1. CPU Requirement\n",
    "    1. Disk needed for PVs (volume of data stored is dictated by settings in MultiCluster Observability CR)\n",
    "    1. Storage needed for Object store (volume of data stored is dictated by settings in MultiCluster Observability CR). This is has many simplifying assumptions.\n",
    "\n",
    "_All calculations are in this notebook below - and they are easy to follow._\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_output_df = pandas.DataFrame(columns=['NumManagedCluster', 'TimeSeriesPerCluster', 'MemoryGB','CPUCorevCPU','PVCGB','ObjStoreGB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(\"..\",\"images\",\"sizing_summary_based_on_max_run_08222023.csv\")\n",
    "master_df = pandas.read_csv(fname,index_col=0)\n",
    "#master_df.info()\n",
    "#master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.5 bytes - 1.5 bytes is maximum range. This is for storage in disk (not memory)\n",
    "observed_bytes_for_storage_per_ts=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulating User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_input(NumMngCluster,TimeSeriesPerCluster):    \n",
    "    number_of_managed_clusters=NumMngCluster\n",
    "    number_of_time_series_per_cluster= TimeSeriesPerCluster\n",
    "\n",
    "    #sampling interval - how frequently do we send data to ACM for storage\n",
    "    number_of_samples_per_hour=12\n",
    "    number_of_hours_pv_retention_hrs=24\n",
    "    number_of_days_for_storage=30\n",
    "\n",
    "    # Present Sizing for Object Store\n",
    "    input_data = {'Specs': ['Number of Managed Clusters', \n",
    "                            'Number of time series per cluster', \n",
    "                            'Number of metric samples per hour',\n",
    "                            'Base Operative time series count',\n",
    "                            'Number of hours of retention in Receiver PV',\n",
    "                            'Target days for storage of data assuming downsampling'], \n",
    "            'Value': [number_of_managed_clusters,number_of_time_series_per_cluster,\n",
    "                      number_of_samples_per_hour,\n",
    "                      number_of_managed_clusters*number_of_time_series_per_cluster,\n",
    "                      number_of_hours_pv_retention_hrs,number_of_days_for_storage]\n",
    "               } \n",
    "\n",
    "    # Create DataFrame \n",
    "    input_df = pandas.DataFrame(input_data) \n",
    "\n",
    "    # Print the output. \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring Memory Requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_memory(master_df,number_of_managed_clusters,number_of_time_series_per_cluster):\n",
    "    print(\"Total Time Series count: \",number_of_managed_clusters*number_of_time_series_per_cluster)\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "    X = master_df['TimeSeriesCount'].values.reshape(-1,1)\n",
    "    Y = master_df['ACMObsMemUsageWSSGB'].values.reshape(-1,1)\n",
    "\n",
    "    regressor = LinearRegression()  \n",
    "    #training the algorithm\n",
    "    regressor.fit(X,Y) \n",
    "    #To retrieve the intercept:\n",
    "    alpha = regressor.intercept_\n",
    "    print(f'alpha = {alpha}')\n",
    "    #For retrieving the slope:\n",
    "    beta = regressor.coef_\n",
    "    print(f'beta = {beta}')\n",
    "\n",
    "    y_pred = alpha + beta*number_of_managed_clusters*number_of_time_series_per_cluster\n",
    "    print(f'A rough Predicted Memory need in GB for {number_of_managed_clusters*number_of_time_series_per_cluster} timeseries - IFF linearity holds = {y_pred} GB')\n",
    "\n",
    "    memory_data = {'Projected Memory (GB)': [y_pred], \n",
    "            'Note': ['This is good of linearity assumption holds']\n",
    "               } \n",
    "  \n",
    "    memory_df = pandas.DataFrame(memory_data) \n",
    "    return memory_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring CPU requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cpu(master_df,number_of_managed_clusters,number_of_time_series_per_cluster):\n",
    "    print(\"Total Time Series count: \",number_of_managed_clusters*number_of_time_series_per_cluster)\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "    X = master_df['TimeSeriesCount'].values.reshape(-1,1)\n",
    "    Y = master_df['ACMObsCPUCoreUsage'].values.reshape(-1,1)\n",
    "\n",
    "    regressor = LinearRegression()  \n",
    "    #training the algorithm\n",
    "    regressor.fit(X,Y) \n",
    "    #To retrieve the intercept:\n",
    "    alpha = regressor.intercept_\n",
    "    print(f'alpha = {alpha}')\n",
    "    #For retrieving the slope:\n",
    "    beta = regressor.coef_\n",
    "    print(f'beta = {beta}')\n",
    "\n",
    "    y_pred = alpha + beta*number_of_managed_clusters*number_of_time_series_per_cluster\n",
    "    print(f'A rough Predicted CPU vCPU needed for {number_of_managed_clusters*number_of_time_series_per_cluster} timeseries - IFF linearity holds = {y_pred} vCPU')\n",
    "\n",
    "    cpu_data = {'Projected CPU  (vCPU)': [y_pred], \n",
    "            'Note': ['This is good of linearity assumption holds']\n",
    "               } \n",
    "  \n",
    "    cpu_df = pandas.DataFrame(cpu_data) \n",
    "    return cpu_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring PVC Requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pvc(number_of_samples_per_hour, number_of_managed_clusters, number_of_time_series_per_cluster,number_of_hours_pv_retention_hrs,observed_bytes_for_storage_per_ts):\n",
    "\n",
    "    #actual_number_of_time_series_local_retention=(\n",
    "    #    actual_number_of_time_series_per_2h*(number_of_hours_pv_retention_hrs/2)\n",
    "    #)\n",
    "\n",
    "    actual_number_of_time_series_local_retention=(\n",
    "    number_of_samples_per_hour*\n",
    "    number_of_managed_clusters*\n",
    "    number_of_time_series_per_cluster*\n",
    "    number_of_hours_pv_retention_hrs\n",
    "    )\n",
    "\n",
    "    inferred_gb_storage_recv=(\n",
    "        (actual_number_of_time_series_local_retention*observed_bytes_for_storage_per_ts)/(1024*1024*1024)\n",
    "    )\n",
    "\n",
    "    inferred_gb_storage_am=10\n",
    "    inferred_gb_storage_compactor=100\n",
    "    inferred_gb_storage_rule=30\n",
    "    inferred_gb_storage_store=100\n",
    "\n",
    "    print(\"Number of hours of retention in Receiver PV: \", number_of_hours_pv_retention_hrs)\n",
    "    print(\"Assumed storage space per time series: \",observed_bytes_for_storage_per_ts, \" bytes\")\n",
    "\n",
    "    # Present Sizing for PVC\n",
    "    pvc_data = {'Pod': ['alertmanager', 'thanos-receiver', \n",
    "                        'thanos-compactor', 'thanos-rule','thanos-store'], \n",
    "            'Number of Replicas': [3,3,1,3,3],\n",
    "            'Sizes Per Replica (GB)': [roundup(inferred_gb_storage_am), \n",
    "                                       roundup(inferred_gb_storage_recv), \n",
    "                                       roundup(inferred_gb_storage_compactor), \n",
    "                                       roundup(inferred_gb_storage_rule),\n",
    "                                       roundup(inferred_gb_storage_store)],\n",
    "            'Total Size (GB)': [roundup(inferred_gb_storage_am)*3, \n",
    "                                roundup(inferred_gb_storage_recv)*3, \n",
    "                                roundup(inferred_gb_storage_compactor)*1, \n",
    "                                roundup(inferred_gb_storage_rule)*3,\n",
    "                                roundup(inferred_gb_storage_store)*3]} \n",
    "\n",
    "    # Create DataFrame \n",
    "    pvc_df = pandas.DataFrame(pvc_data) \n",
    "\n",
    "    # Print the output. \n",
    "    return pvc_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring Object Store Requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_objstore(number_of_samples_per_hour, number_of_managed_clusters, number_of_time_series_per_cluster,number_of_hours_pv_retention_hrs,observed_bytes_for_storage_per_ts,number_of_days_for_storage):\n",
    "    \n",
    "    actual_number_of_time_series_local_retention=(\n",
    "    number_of_samples_per_hour*\n",
    "    number_of_managed_clusters*\n",
    "    number_of_time_series_per_cluster*\n",
    "    number_of_hours_pv_retention_hrs\n",
    "    )\n",
    "\n",
    "    inferred_gb_storage_recv=(\n",
    "        (actual_number_of_time_series_local_retention*observed_bytes_for_storage_per_ts)/(1024*1024*1024)\n",
    "    )\n",
    "    \n",
    "    #this is needed just in case the number_of_hours_pv_retention_hrs is set to something other than 24hrs\n",
    "    inferred_obj_storage_per_day=inferred_gb_storage_recv*24/number_of_hours_pv_retention_hrs\n",
    "    inferred_obj_storage_for_life=inferred_obj_storage_per_day*number_of_days_for_storage*3\n",
    "\n",
    "    print(\"Target days for storage of data assuming downsampling: \", number_of_days_for_storage)\n",
    "    print(\"Inferred Object Storage needed: \", inferred_obj_storage_for_life , \"GB\")\n",
    "\n",
    "    # Present Sizing for Object Store\n",
    "    obj_data = {'Steps': ['Raw storage for 1 day (GB)', \n",
    "                          'number of days to be stored', \n",
    "                          'Raw storage for all days (GB)', \n",
    "                          '5m storage for all days (GB)',\n",
    "                          '1hr storage for all days (GB)',\n",
    "                          'Total storage for all days (GB)'], \n",
    "            'Value': [inferred_obj_storage_per_day,\n",
    "                      number_of_days_for_storage,\n",
    "                      inferred_obj_storage_for_life/3,\n",
    "                      inferred_obj_storage_for_life/3,\n",
    "                      inferred_obj_storage_for_life/3,\n",
    "                      roundup(inferred_obj_storage_for_life)]\n",
    "               } \n",
    "\n",
    "    # Create DataFrame \n",
    "    objstore_df = pandas.DataFrame(obj_data) \n",
    "\n",
    "    # Print the output. \n",
    "    return objstore_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_date(input_df,memory_df,cpu_df,pvc_df,objstore_df):\n",
    "    cluster=input_df['Value'][0]\n",
    "    ts=input_df['Value'][1]\n",
    "    mem=float(memory_df['Projected Memory (GB)'][0][0])\n",
    "    cpu=float(cpu_df['Projected CPU  (vCPU)'][0][0])\n",
    "    pvc=pvc_df['Total Size (GB)'].sum()\n",
    "    objstore=objstore_df['Value'].iloc[5]\n",
    "    return cluster,ts,mem,cpu,pvc,objstore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical Input Parameters to Size\n",
    ">- **Please tweak these values as needed**\n",
    ">- *Our calculations are solely based on what you input below*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pandas.DataFrame({\n",
    "    'NumMngCluster': [150, 300, 3000],\n",
    "    'TimeSeriesPerCluster': [4500, 4500, 4500]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Final Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for : 150\n",
      "                                               Specs   Value\n",
      "0                         Number of Managed Clusters     150\n",
      "1                  Number of time series per cluster    4500\n",
      "2                  Number of metric samples per hour      12\n",
      "3                   Base Operative time series count  675000\n",
      "4        Number of hours of retention in Receiver PV      24\n",
      "5  Target days for storage of data assuming downs...      30\n",
      "Total Time Series count:  675000\n",
      "------------------------------------\n",
      "alpha = [-3.78028111]\n",
      "beta = [[1.60469085e-05]]\n",
      "A rough Predicted Memory need in GB for 675000 timeseries - IFF linearity holds = [[7.0513821]] GB\n",
      "Total Time Series count:  675000\n",
      "------------------------------------\n",
      "alpha = [0.56711376]\n",
      "beta = [[2.83832114e-06]]\n",
      "A rough Predicted CPU vCPU needed for 675000 timeseries - IFF linearity holds = [[2.48298053]] vCPU\n",
      "Number of hours of retention in Receiver PV:  24\n",
      "Assumed storage space per time series:  2  bytes\n",
      "Target days for storage of data assuming downsampling:  30\n",
      "Inferred Object Storage needed:  32.588839530944824 GB\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "for : 300\n",
      "                                               Specs    Value\n",
      "0                         Number of Managed Clusters      300\n",
      "1                  Number of time series per cluster     4500\n",
      "2                  Number of metric samples per hour       12\n",
      "3                   Base Operative time series count  1350000\n",
      "4        Number of hours of retention in Receiver PV       24\n",
      "5  Target days for storage of data assuming downs...       30\n",
      "Total Time Series count:  1350000\n",
      "------------------------------------\n",
      "alpha = [-3.78028111]\n",
      "beta = [[1.60469085e-05]]\n",
      "A rough Predicted Memory need in GB for 1350000 timeseries - IFF linearity holds = [[17.8830453]] GB\n",
      "Total Time Series count:  1350000\n",
      "------------------------------------\n",
      "alpha = [0.56711376]\n",
      "beta = [[2.83832114e-06]]\n",
      "A rough Predicted CPU vCPU needed for 1350000 timeseries - IFF linearity holds = [[4.39884729]] vCPU\n",
      "Number of hours of retention in Receiver PV:  24\n",
      "Assumed storage space per time series:  2  bytes\n",
      "Target days for storage of data assuming downsampling:  30\n",
      "Inferred Object Storage needed:  65.17767906188965 GB\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "for : 3000\n",
      "                                               Specs     Value\n",
      "0                         Number of Managed Clusters      3000\n",
      "1                  Number of time series per cluster      4500\n",
      "2                  Number of metric samples per hour        12\n",
      "3                   Base Operative time series count  13500000\n",
      "4        Number of hours of retention in Receiver PV        24\n",
      "5  Target days for storage of data assuming downs...        30\n",
      "Total Time Series count:  13500000\n",
      "------------------------------------\n",
      "alpha = [-3.78028111]\n",
      "beta = [[1.60469085e-05]]\n",
      "A rough Predicted Memory need in GB for 13500000 timeseries - IFF linearity holds = [[212.852983]] GB\n",
      "Total Time Series count:  13500000\n",
      "------------------------------------\n",
      "alpha = [0.56711376]\n",
      "beta = [[2.83832114e-06]]\n",
      "A rough Predicted CPU vCPU needed for 13500000 timeseries - IFF linearity holds = [[38.88444911]] vCPU\n",
      "Number of hours of retention in Receiver PV:  24\n",
      "Assumed storage space per time series:  2  bytes\n",
      "Target days for storage of data assuming downsampling:  30\n",
      "Inferred Object Storage needed:  651.7767906188965 GB\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/61/bmwhyqss6rjdhl37b33pbz9m0000gn/T/ipykernel_59202/1820612685.py:4: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  mem=float(memory_df['Projected Memory (GB)'][0][0])\n",
      "/var/folders/61/bmwhyqss6rjdhl37b33pbz9m0000gn/T/ipykernel_59202/1820612685.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  cpu=float(cpu_df['Projected CPU  (vCPU)'][0][0])\n",
      "/var/folders/61/bmwhyqss6rjdhl37b33pbz9m0000gn/T/ipykernel_59202/2676560851.py:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  master_output_df = master_output_df._append({'NumManagedCluster': cluster, 'TimeSeriesPerCluster': ts, 'MemoryGB': mem,'CPUCorevCPU':cpu,'PVCGB':pvc,'ObjStoreGB':objstore}, ignore_index=True)\n",
      "/var/folders/61/bmwhyqss6rjdhl37b33pbz9m0000gn/T/ipykernel_59202/1820612685.py:4: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  mem=float(memory_df['Projected Memory (GB)'][0][0])\n",
      "/var/folders/61/bmwhyqss6rjdhl37b33pbz9m0000gn/T/ipykernel_59202/1820612685.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  cpu=float(cpu_df['Projected CPU  (vCPU)'][0][0])\n",
      "/var/folders/61/bmwhyqss6rjdhl37b33pbz9m0000gn/T/ipykernel_59202/1820612685.py:4: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  mem=float(memory_df['Projected Memory (GB)'][0][0])\n",
      "/var/folders/61/bmwhyqss6rjdhl37b33pbz9m0000gn/T/ipykernel_59202/1820612685.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  cpu=float(cpu_df['Projected CPU  (vCPU)'][0][0])\n"
     ]
    }
   ],
   "source": [
    "for index, row in customer_df.iterrows():\n",
    "    print(\"for :\"  , row['NumMngCluster'] )\n",
    "    input_df=set_input(row['NumMngCluster'],row['TimeSeriesPerCluster'])\n",
    "    \n",
    "    print(input_df)\n",
    "\n",
    "    memory_df = calculate_memory(master_df,row['NumMngCluster'],row['TimeSeriesPerCluster'])\n",
    "    cpu_df = calculate_cpu(master_df,row['NumMngCluster'],row['TimeSeriesPerCluster'])\n",
    "    pvc_df = calculate_pvc(input_df['Value'][2], input_df['Value'][0], input_df['Value'][1],input_df['Value'][4],observed_bytes_for_storage_per_ts)\n",
    "    objstore_df = calculate_objstore(input_df['Value'][2], input_df['Value'][0], input_df['Value'][1],input_df['Value'][4],observed_bytes_for_storage_per_ts,input_df['Value'][5])\n",
    "    cluster,ts,mem,cpu,pvc,objstore=save_date(input_df,memory_df,cpu_df,pvc_df,objstore_df)\n",
    "    master_output_df = master_output_df._append({'NumManagedCluster': cluster, 'TimeSeriesPerCluster': ts, 'MemoryGB': mem,'CPUCorevCPU':cpu,'PVCGB':pvc,'ObjStoreGB':objstore}, ignore_index=True)\n",
    "    print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "#print(\"--------------- Final Results ------------\")\n",
    "#print(master_output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumManagedCluster</th>\n",
       "      <th>TimeSeriesPerCluster</th>\n",
       "      <th>MemoryGB</th>\n",
       "      <th>CPUCorevCPU</th>\n",
       "      <th>PVCGB</th>\n",
       "      <th>ObjStoreGB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>7.051382</td>\n",
       "      <td>2.482981</td>\n",
       "      <td>550.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>17.883045</td>\n",
       "      <td>4.398847</td>\n",
       "      <td>550.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>212.852983</td>\n",
       "      <td>38.884449</td>\n",
       "      <td>550.0</td>\n",
       "      <td>660.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumManagedCluster  TimeSeriesPerCluster    MemoryGB  CPUCorevCPU  PVCGB  \\\n",
       "0              150.0                4500.0    7.051382     2.482981  550.0   \n",
       "1              300.0                4500.0   17.883045     4.398847  550.0   \n",
       "2             3000.0                4500.0  212.852983    38.884449  550.0   \n",
       "\n",
       "   ObjStoreGB  \n",
       "0        40.0  \n",
       "1        70.0  \n",
       "2       660.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master_outputDF saved..\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    master_output_df.to_csv('../output/master_output.csv', index = True, header=True)  \n",
    "    print(\"master_outputDF saved..\")\n",
    "except Exception as e:\n",
    "    print(\"Failure in saving master_outputDF: \",e)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
